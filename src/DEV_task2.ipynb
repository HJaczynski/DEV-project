{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fed25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import clustbench\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from genieclust import Genie\n",
    "from sklearn.metrics import (adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score, silhouette_score, calinski_harabasz_score, davies_bouldin_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/gagolews/clustering-data-v1/raw/v1.1.0\"\n",
    "datasets = ['a1', 'a2', 'a3', 'aggregation', 'compound', 'd31', 'r15', 'flame', 'jain', 'pathbased', 'spiral', 's1', 's2', 's3', 's4', 'unbalance']\n",
    "\n",
    "genie_params = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "agg_linkages = ['single', 'average', 'complete', 'ward']\n",
    "dbscan_params = {'eps': [0.1, 0.2, 0.3, 0.5], 'min_samples': [3, 5, 10]}\n",
    "\n",
    "metrics = ['ARI', 'NMI', 'FMI', 'Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin']  ##https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c304263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(X, y_true, clustering_algorithm):\n",
    "    \"\"\"\n",
    "    Evaluate clustering performance using various metrics,\n",
    "    but only compute Silhouette / Calinski-Harabasz / Davies-Bouldin\n",
    "    if there are at least 2 clusters.\n",
    "    \"\"\"\n",
    "    y_pred = clustering_algorithm.fit_predict(X)\n",
    "\n",
    "    results = {\n",
    "        'ARI': adjusted_rand_score(y_true, y_pred),\n",
    "        'NMI': normalized_mutual_info_score(y_true, y_pred),\n",
    "        'FMI': fowlkes_mallows_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    # count unique labels to chech if we can compute the other metrics\n",
    "    n_labels = len(np.unique(y_pred))\n",
    "    if n_labels >= 2 and n_labels <= len(y_pred) - 1:\n",
    "        results['Silhouette']          = silhouette_score(X, y_pred)\n",
    "        results['Calinski-Harabasz']   = calinski_harabasz_score(X, y_pred)\n",
    "        results['Davies-Bouldin']      = davies_bouldin_score(X, y_pred)\n",
    "    else:\n",
    "        results['Silhouette']          = np.nan\n",
    "        results['Calinski-Harabasz']   = np.nan\n",
    "        results['Davies-Bouldin']      = np.nan\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21bfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(X, y_true, n_clusters):\n",
    "    records = []\n",
    "    \n",
    "    # KMeans, with three different seeds\n",
    "    for i in range(3):\n",
    "        seed = 42 + i\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=seed)\n",
    "        metrics = evaluate_clustering(X, y_true, kmeans)\n",
    "        record = {\n",
    "            'method': 'KMeans',\n",
    "            'param': f'random_state={seed}',\n",
    "            **metrics\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    # Agglomerative Clustering\n",
    "    for linkage in agg_linkages:\n",
    "        agg = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "        metrics = evaluate_clustering(X, y_true, agg)\n",
    "        records.append({\n",
    "            'method': 'Agglomerative',\n",
    "            'param': f'linkage={linkage}',\n",
    "            **metrics\n",
    "        })\n",
    "    \n",
    "    # DBSCAN\n",
    "    for eps in dbscan_params['eps']:\n",
    "        for min_samples in dbscan_params['min_samples']:\n",
    "            dbs = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            metrics = evaluate_clustering(X, y_true, dbs)\n",
    "            records.append({\n",
    "                'method': 'DBSCAN',\n",
    "                'param': f'eps={eps}, min_samples={min_samples}',\n",
    "                **metrics\n",
    "            })\n",
    "    \n",
    "    # Genie\n",
    "    for t in genie_params:\n",
    "        genie = Genie(n_clusters=n_clusters)\n",
    "        metrics = evaluate_clustering(X, y_true, genie)\n",
    "        records.append({\n",
    "            'method': 'Genie',\n",
    "            'param': f'threshold={t}',\n",
    "            **metrics\n",
    "        })\n",
    "    \n",
    "    # turn the list of dicts into a DataFrame\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68917e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset processing: a1\n",
      "Dataset processing: a2\n",
      "Dataset processing: a3\n",
      "Dataset processing: aggregation\n",
      "Dataset processing: compound\n",
      "Dataset processing: d31\n",
      "Dataset processing: r15\n",
      "Dataset processing: flame\n",
      "Dataset processing: jain\n",
      "Dataset processing: pathbased\n",
      "Dataset processing: spiral\n",
      "Dataset processing: s1\n",
      "Dataset processing: s2\n",
      "Dataset processing: s3\n",
      "Dataset processing: s4\n",
      "Dataset processing: unbalance\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./benchmark_metrics.csv'):\n",
    "    all_results = []\n",
    "\n",
    "    for name in datasets:\n",
    "        print(f\"Dataset processing: {name}\")\n",
    "        bench = clustbench.load_dataset('sipu', name, url = data_url)\n",
    "        X, y_true = bench.data, bench.labels[0]\n",
    "        n_clusters = bench.n_clusters[0]\n",
    "        df_res = run_experiment(X, y_true, n_clusters)   # now a DataFrame\n",
    "        for entry in df_res.to_dict('records'):\n",
    "            entry['dataset'] = name\n",
    "            all_results.append(entry)\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv('benchmark_metrics.csv', index = False)\n",
    "else:\n",
    "    df = pd.read_csv('./benchmark_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b66ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
